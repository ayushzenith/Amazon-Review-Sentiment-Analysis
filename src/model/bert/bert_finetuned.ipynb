{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Amazon Fashion Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  df = {}\n",
    "  for i, d in enumerate(parse(path)):\n",
    "    df[i] = d\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('../../../data/raw/AMAZON_FASHION.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   overall  verified   reviewTime      reviewerID        asin  reviewerName  \\\n0      5.0      True  10 20, 2014  A1D4G1SNUZWQOT  7106116521         Tracy   \n1      2.0      True  09 28, 2014  A3DDWDH9PX2YX2  7106116521     Sonja Lau   \n2      4.0     False  08 25, 2014  A2MWC41EW7XL15  7106116521      Kathleen   \n3      2.0      True  08 24, 2014  A2UH2QQ275NV45  7106116521   Jodi Stoner   \n4      3.0     False  07 27, 2014   A89F3LQADZBS5  7106116521  Alexander D.   \n\n                                          reviewText  \\\n0                             Exactly what I needed.   \n1  I agree with the other review, the opening is ...   \n2  Love these... I am going to order another pack...   \n3                                too tiny an opening   \n4                                               Okay   \n\n                                             summary  unixReviewTime vote  \\\n0                             perfect replacements!!      1413763200  NaN   \n1  I agree with the other review, the opening is ...      1411862400    3   \n2                                My New 'Friends' !!      1408924800  NaN   \n3                                          Two Stars      1408838400  NaN   \n4                                        Three Stars      1406419200  NaN   \n\n  style image  overallInt                                         reviewFull  \n0   NaN   NaN           5      Exactly what I needed. perfect replacements!!  \n1   NaN   NaN           2  I agree with the other review, the opening is ...  \n2   NaN   NaN           4  Love these... I am going to order another pack...  \n3   NaN   NaN           2                      too tiny an opening Two Stars  \n4   NaN   NaN           3                                   Okay Three Stars  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>verified</th>\n      <th>reviewTime</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewerName</th>\n      <th>reviewText</th>\n      <th>summary</th>\n      <th>unixReviewTime</th>\n      <th>vote</th>\n      <th>style</th>\n      <th>image</th>\n      <th>overallInt</th>\n      <th>reviewFull</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.0</td>\n      <td>True</td>\n      <td>10 20, 2014</td>\n      <td>A1D4G1SNUZWQOT</td>\n      <td>7106116521</td>\n      <td>Tracy</td>\n      <td>Exactly what I needed.</td>\n      <td>perfect replacements!!</td>\n      <td>1413763200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>Exactly what I needed. perfect replacements!!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>True</td>\n      <td>09 28, 2014</td>\n      <td>A3DDWDH9PX2YX2</td>\n      <td>7106116521</td>\n      <td>Sonja Lau</td>\n      <td>I agree with the other review, the opening is ...</td>\n      <td>I agree with the other review, the opening is ...</td>\n      <td>1411862400</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>I agree with the other review, the opening is ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>08 25, 2014</td>\n      <td>A2MWC41EW7XL15</td>\n      <td>7106116521</td>\n      <td>Kathleen</td>\n      <td>Love these... I am going to order another pack...</td>\n      <td>My New 'Friends' !!</td>\n      <td>1408924800</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>Love these... I am going to order another pack...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>True</td>\n      <td>08 24, 2014</td>\n      <td>A2UH2QQ275NV45</td>\n      <td>7106116521</td>\n      <td>Jodi Stoner</td>\n      <td>too tiny an opening</td>\n      <td>Two Stars</td>\n      <td>1408838400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>too tiny an opening Two Stars</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.0</td>\n      <td>False</td>\n      <td>07 27, 2014</td>\n      <td>A89F3LQADZBS5</td>\n      <td>7106116521</td>\n      <td>Alexander D.</td>\n      <td>Okay</td>\n      <td>Three Stars</td>\n      <td>1406419200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>Okay Three Stars</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate reviews\n",
    "# Any review with the same reviewText, reviewerID, overall ranking is considered a duplicate\n",
    "df = df.drop_duplicates(subset=['reviewText', 'reviewerID', 'overall'])\n",
    "\n",
    "# Drop reviews with no reviewText since we are primarily interested in analyzing review text\n",
    "df = df.dropna(subset=['reviewText'])\n",
    "\n",
    "df['overallInt'] = df['overall'].astype(int)\n",
    "df['reviewFull'] = df['reviewText'] + ' ' + df['summary']\n",
    "df['reviewFull'] = df['reviewFull'].astype(str)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          reviewText  \\\n0                             Exactly what I needed.   \n1  I agree with the other review, the opening is ...   \n2  Love these... I am going to order another pack...   \n3                                too tiny an opening   \n4                                               Okay   \n\n                                             summary  overall  overallInt  \\\n0                             perfect replacements!!      5.0           5   \n1  I agree with the other review, the opening is ...      2.0           2   \n2                                My New 'Friends' !!      4.0           4   \n3                                          Two Stars      2.0           2   \n4                                        Three Stars      3.0           3   \n\n                                          reviewFull  \n0      Exactly what I needed. perfect replacements!!  \n1  I agree with the other review, the opening is ...  \n2  Love these... I am going to order another pack...  \n3                      too tiny an opening Two Stars  \n4                                   Okay Three Stars  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n      <th>summary</th>\n      <th>overall</th>\n      <th>overallInt</th>\n      <th>reviewFull</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Exactly what I needed.</td>\n      <td>perfect replacements!!</td>\n      <td>5.0</td>\n      <td>5</td>\n      <td>Exactly what I needed. perfect replacements!!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I agree with the other review, the opening is ...</td>\n      <td>I agree with the other review, the opening is ...</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>I agree with the other review, the opening is ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Love these... I am going to order another pack...</td>\n      <td>My New 'Friends' !!</td>\n      <td>4.0</td>\n      <td>4</td>\n      <td>Love these... I am going to order another pack...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>too tiny an opening</td>\n      <td>Two Stars</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>too tiny an opening Two Stars</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Okay</td>\n      <td>Three Stars</td>\n      <td>3.0</td>\n      <td>3</td>\n      <td>Okay Three Stars</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep relevant columns\n",
    "df = df[['reviewText', 'summary', 'overall', 'overallInt', 'reviewFull']]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "df.to_csv('../../../data/processed/bert_amazon_fashion.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert for Amazon Fashion Dataset star rating prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5}\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, BertModel, BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('../../../data/processed/bert_amazon_fashion.csv')\n",
    "\n",
    "print(set(df['overallInt']))\n",
    "df['overallInt'] = df['overallInt'].apply(lambda x: x - 1)\n",
    "print(set(df['overallInt']))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class AmazonFashionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        review = row['reviewFull']\n",
    "        rating = row['overallInt']\n",
    "\n",
    "        review = str(review)\n",
    "        rating = int(rating)\n",
    "\n",
    "        inputs = self.tokenizer(review, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(0)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "        inputs['labels'] = torch.tensor(rating)\n",
    "        return inputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\anaconda3\\envs\\rss-hw4\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'classifier.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = AmazonFashionDataset(train_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = AmazonFashionDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170348 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4])\n",
      "tensor([4, 1, 4, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample batch\n",
    "for batch in tqdm(train_loader):\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['input_ids'][0].shape)\n",
    "    print(batch['input_ids'][1].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    print(batch['labels'])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([512])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['input_ids'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def evaluate(model, test):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1) + 1\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 86580/170348 [4:26:16<4:30:57,  5.15it/s] "
     ]
    }
   ],
   "source": [
    "accumulation_steps = 4\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / accumulation_steps  # Normalize our loss (if averaged)\n",
    "        loss.backward()\n",
    "\n",
    "        if (i+1) % accumulation_steps == 0:  # Wait for several backward steps\n",
    "            optimizer.step()  # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "    print(\"Test Set: Accuracy\", str(evaluate(model, test_loader)))\n",
    "    model.save_pretrained('../../../models/bert_amazon_fashion_epoch' + str(epoch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained('../../../models/bert_amazon_fashion')\n",
    "tokenizer.save_pretrained('../../../models/bert_amazon_fashion_tokenizer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Accuracy\", str(evaluate(model, test_loader)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_rating(review, model, tokenizer):\n",
    "    inputs = tokenizer(review, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1) + 1\n",
    "    return prediction.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Conclusion\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
